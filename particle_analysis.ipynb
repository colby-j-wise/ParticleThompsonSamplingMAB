{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n users: 943\n",
      "n items: 1650\n"
     ]
    }
   ],
   "source": [
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "datapoints = np.array([[row[0], row[1], row[2], row[3]] for row in datapoints])\n",
    "np.random.shuffle(datapoints)\n",
    "\n",
    "user_ids = set([datapoint[0] for datapoint in datapoints])\n",
    "n_users = len(user_ids)\n",
    "max_user = max(user_ids) + 1\n",
    "print(\"n users:\", n_users)\n",
    "items_ids = set([datapoint[1] for datapoint in datapoints])\n",
    "n_items = len(items_ids)\n",
    "max_item = max(items_ids) + 1\n",
    "print(\"n items:\", n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # subtract max value to prevent overflow\\n\"\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (np.random.random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "def systematic_resample(weights):\n",
    "    N = len(weights)\n",
    "\n",
    "    # make N subdivisions, and choose positions with a consistent random offset\n",
    "    positions = (np.random.random() + np.arange(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared error: 3.37\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cae8026d4f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"v\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;34m\"var_u\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparticles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"var_u\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \"var_i\": particles[d][1][\"var_i\"]} for d in ds]\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparticle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_u_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta_u_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ds' is not defined"
     ]
    }
   ],
   "source": [
    "n_particles = 10\n",
    "k = 3\n",
    "var = 0.5\n",
    "particles = [(1 / n_particles, {\"u\": np.random.normal(size=(max_user, k)),\n",
    "                                \"v\": np.random.normal(size=(max_item, k)),\n",
    "                                \"var_u\": 1.0,\n",
    "                                \"var_i\": 1.0}) for _ in range(n_particles)]\n",
    "\n",
    "# get mean rating to make rating data centered at 0\n",
    "mean_rating = np.mean(datapoints[:, 2])\n",
    "data_store = {u_id: {row[1]: row[2] - mean_rating for row in datapoints if row[0] == u_id} for u_id in user_ids}\n",
    "\n",
    "user_history = {} # user_rating_history[user_id][\"item_ids\"], user_rating_history[user_id][\"ratings\"]\n",
    "item_history = {} # item_rating_history[item_id][\"user_ids\"], item_rating_history[item_id][\"ratings\"]\n",
    "ses = []\n",
    "ctr = 0\n",
    "ctr_hist = []\n",
    "uniParticlesList = []\n",
    "multi, strat, system = [], [], []\n",
    "\n",
    "# what we're going to do here is assume that OUR system made the item recommendation and is observing the\n",
    "# rating we have in the dataset ... it's sort of like we're starting at line 11\n",
    "for _idx in range(len(datapoints)):\n",
    "\n",
    "    # randomly get a user\n",
    "    user_id = np.random.choice([i for i in data_store.keys()])\n",
    "    user_items = [i for i in data_store[user_id].keys()]\n",
    "    \n",
    "    # highest rating this user has\n",
    "    highest_rating = max(data_store[user_id].values())\n",
    "    # get highest rated items\n",
    "    highest_rated_items = [x for x in data_store[user_id].keys() if data_store[user_id][x] >= highest_rating]\n",
    "    \n",
    "    # get indices for items this user rated\n",
    "    indices = np.array(user_items)\n",
    "    \n",
    "    # randomly select a particle\n",
    "    random_particle = np.random.choice(range(n_particles))\n",
    "    particle = particles[random_particle]\n",
    "    \n",
    "    # predict a rating only for the items rated by that user\n",
    "    predicted_rating = np.dot(particle[1][\"u\"][user_id, :], particle[1][\"v\"][indices, :].T)\n",
    "    \n",
    "    # get the item id\n",
    "    max_rating_ind = np.argmax(predicted_rating)\n",
    "    item_id = [i for i in data_store[user_id].keys()][max_rating_ind]\n",
    "    \n",
    "    # add to ctr if possible\n",
    "    if item_id in highest_rated_items:\n",
    "        ctr += 1\n",
    "    ctr_hist.append(ctr / (_idx + 1))\n",
    "    \n",
    "    # get the true rating\n",
    "    rating = data_store[user_id][item_id]\n",
    "    \n",
    "    # delete this item from this user\n",
    "    del data_store[user_id][item_id]\n",
    "    \n",
    "    # delete the user from the data store if they have no reviews left\n",
    "    if not data_store[user_id]:\n",
    "        del data_store[user_id]\n",
    "        \n",
    "    error = predicted_rating[max_rating_ind] - rating\n",
    "    se = error ** 2\n",
    "    if _idx % 101 == 0:\n",
    "        ses.append(se)\n",
    "    if _idx % 1000== 0:\n",
    "        print(\"squared error: {:.2f}\".format(se))\n",
    "\n",
    "    # line 17\n",
    "    precision_u_i = []\n",
    "    eta_u_i = []\n",
    "    for particle in particles:\n",
    "        if user_id not in user_history:\n",
    "            precision_u_i.append(np.eye(k))\n",
    "            eta_u_i.append(np.zeros(k))\n",
    "        else:\n",
    "            v_j = particle[1][\"v\"][user_history[user_id][\"item_ids\"], :]\n",
    "            lambda_u_i = 1 / var * \\\n",
    "                np.dot(v_j.T, v_j) + \\\n",
    "                1 / particle[1][\"var_u\"] * np.eye(k)\n",
    "\n",
    "            precision_u_i.append(lambda_u_i)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    v_j,\n",
    "                    np.array(user_history[user_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "            eta_u_i.append(eta.reshape(-1))\n",
    "\n",
    "    # line 18\n",
    "    weights = []\n",
    "    mus = [1 / var * np.dot(np.linalg.inv(lambda_), eta) for lambda_, eta in zip(precision_u_i, eta_u_i)]\n",
    "    for particle, mu, precision in zip(particles, mus, precision_u_i):\n",
    "        v_j = particle[1][\"v\"][item_id, :]\n",
    "        cov = 1 / var + np.dot(np.dot(v_j.T, precision), v_j)\n",
    "        w = np.random.normal(\n",
    "            np.dot(v_j.T, mu),\n",
    "            cov\n",
    "        )\n",
    "        weights.append(w)\n",
    "    normalized_weights = softmax(weights)\n",
    "\n",
    "    # line 19\n",
    "    eff_threshold = 4\n",
    "    Neff = 1/sum([i**2 for i in normalized_weights])\n",
    "    #print(Neff)\n",
    "    #if not all(weight > 1./n_particles for weight in normalized_weights):\n",
    "    if Neff >= eff_threshold:\n",
    "        #print(\"resampling...\")\n",
    "        ds = [np.random.choice(range(n_particles), p=normalized_weights) for _ in range(n_particles)]\n",
    "\n",
    "        # Capture # of particles. Just test to see what stratfied/systematic resampling gives\n",
    "        str_idx = stratified_resample(normalized_weights)\n",
    "        sys_idx = systematic_resample(normalized_weights)\n",
    "        multi.append(len(set(ds)))\n",
    "        strat.append(len(set(str_idx)))\n",
    "        system.append(len(set(sys_idx)))\n",
    "\n",
    "    p_prime = [{\"u\": np.copy(particles[d][1][\"u\"]),\n",
    "                \"v\": np.copy(particles[d][1][\"v\"]),\n",
    "                \"var_u\": particles[d][1][\"var_u\"],\n",
    "                \"var_i\": particles[d][1][\"var_i\"]} for d in ds]\n",
    "        \n",
    "    for idx, (particle, precision, e) in enumerate(zip(p_prime, precision_u_i, eta_u_i)):\n",
    "\n",
    "        # line 21\n",
    "        v_j = particle[\"v\"][item_id, :]\n",
    "        add_to_precision = 1 / var * np.dot(v_j.reshape(-1, 1), v_j.reshape(1, -1))\n",
    "        precision += add_to_precision\n",
    "\n",
    "        add_to_eta = rating * v_j\n",
    "        e += add_to_eta\n",
    "\n",
    "        # line 22\n",
    "        sampled_user_vector = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision), e),\n",
    "            np.linalg.inv(precision)\n",
    "        )\n",
    "        \n",
    "        p_prime[idx][\"u\"][user_id, :] = sampled_user_vector\n",
    "\n",
    "        # line 24\n",
    "        if item_id not in item_history:\n",
    "            precision_v_i = np.eye(k)\n",
    "            eta = np.zeros(k)\n",
    "        else:\n",
    "            u_i = particle[\"u\"][item_history[item_id][\"user_ids\"], :]\n",
    "            precision_v_i = 1 / var * \\\n",
    "                np.dot(u_i.T, u_i) + \\\n",
    "                1 / particle[\"var_i\"] * np.eye(k)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    u_i,\n",
    "                    np.array(item_history[item_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "        # line 25\n",
    "        item_sample = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision_v_i), eta),\n",
    "            np.linalg.inv(precision_v_i)\n",
    "        )\n",
    "        p_prime[idx][\"v\"][item_id, :] = item_sample\n",
    "        \n",
    "    particles = [(1 / n_particles, particle) for particle in p_prime]\n",
    "\n",
    "    if user_id not in user_history:\n",
    "        user_history[user_id] = {\"item_ids\": [], \"ratings\": []}\n",
    "    if item_id not in item_history:\n",
    "        item_history[item_id] = {\"user_ids\": [], \"ratings\": []}\n",
    "    user_history[user_id][\"item_ids\"].append(item_id)\n",
    "    user_history[user_id][\"ratings\"].append(rating)\n",
    "    item_history[item_id][\"user_ids\"].append(user_id)\n",
    "    item_history[item_id][\"ratings\"].append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg num particles Multinomial:\", round(np.mean(multi),2) ) # On average looks like multi uses less particles\n",
    "print(\"Avg num particles Stratfied:\", round(np.mean(strat),2) ) # Followed slightly by stratefied sampling\n",
    "print(\"Avg num particles Systematic:\", round(np.mean(system),2) ) # Systematic uses most -- wonder if should change from multi above to systematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(multi)), multi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_particles(p_hist, interval=4):\n",
    "    avg = []\n",
    "    for i in range(0,len(strat)-1, interval):\n",
    "        avg.append(sum(strat[i : i + interval])/interval)\n",
    "    return avg\n",
    "\n",
    "# Need to make this just a smoothed line graph. Line graph isn't showing same result\n",
    "def plot_avgParticles(avg, n_particles=n_particles, k=k):\n",
    "    plt.plot(range(len(avg)), avg)\n",
    "    plt.title(\"Final Particle Number: {:.0f} | Particles Num = {}, K = {}\".format(avg[-1], n_particles, k))\n",
    "    plt.xlabel('resample step')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = np.array(multi)\n",
    "strat = np.array(strat)\n",
    "system = np.array(system)\n",
    "\n",
    "# Plot of multinomial average every 2 iters\n",
    "# Like how it shows we use ~5 particles then -> 1\n",
    "avg = average_particles(multi, 2) \n",
    "plot_avgParticles(avg)\n",
    "# Plot of stratfied resampling every  10 iters\n",
    "avg = average_particles(strat, 10) \n",
    "plot_avgParticles(avg)\n",
    "# Plot of systematic resampling every  100 iters\n",
    "# Smooth line but seems like only use ~2 -> 1\n",
    "avg = average_particles(system, 100) \n",
    "plot_avgParticles(avg)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x):\n",
    "    avgs = []\n",
    "    for i, v in enumerate(x):\n",
    "        avgs.append(np.sum(x[:i]) / i)\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(ses)\n",
    "plt.plot(range(len(out)), out)\n",
    "plt.title(\"Average Particles: {:.2f} | Particles={} K={}\".format(out[-1], n_particles, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(range(len(ctr_hist)), ctr_hist)\n",
    "# plt.title(\"cumulative take rate {:.2f}\".format(ctr_hist[-1]))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
