# Multi-Arm Bandits using Thompson Sampling and Probabilistic Matrix Factorization

### Final project for Advance Machine Learning for Personalization (COMS6998), Columbia Spring 2018. We explore the Multi-Arm Bandit problem for online recommendations by combining Particle Thompson Sampling with Probabilistic Matrix-Factorization for ratings-based datasets.

### References:
##### [1] Efficient Thompson Sampling for Online Matrix-Factorization Recommendation (Kawale et. al., 2015)
##### [2] Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms (Wang et. al., 2017)
##### [3] Interactive Collaborative Filtering (Zhao et. al., 2013)
##### [4] Learning to Optimize Via Posterior Sampling (Daniel Russo, Benjamin Roy. 2014)
##### [5] Sequential Monte Carlo Bandits (Michael Cherkassky, Luke Bornn. 2013)
##### [6] Analysis of Thompson Sampling for the Multi-armed Bandit Problem (Agrawal, Goyal 2012)
##### [7] A Tutorial on Thompson Sampling (Russo, et. al 2018)

### Team: 
#### Michael Alvarino, Bharat Srikishan, Colby Wise @Columbia.edu 

### Keywords: 
##### Particle Filtering, Thompson Sampling, Probabilistic Matrix Factorization, Bayesian Hierarchical Multi-Arm Bandits


![Image of Squirrel](http://mrbesilly.typepad.com/.a/6a00d8341bfa6953ef01310fe97dc4970c-320wi)