{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n users: 943\n",
      "n items: 1650\n"
     ]
    }
   ],
   "source": [
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "datapoints = np.array([[row[0], row[1], row[2], row[3]] for row in datapoints])\n",
    "np.random.shuffle(datapoints)\n",
    "\n",
    "# Used in precision eval... not rly needed given datapoints\n",
    "user_items = defaultdict(list)\n",
    "for row in datapoints:\n",
    "    user_items[row[0]].append(row[1])\n",
    "\n",
    "user_ids = set([datapoint[0] for datapoint in datapoints])\n",
    "n_users = len(user_ids)\n",
    "max_user = max(user_ids) + 1\n",
    "print(\"n users:\", n_users)\n",
    "items_ids = set([datapoint[1] for datapoint in datapoints])\n",
    "n_items = len(items_ids)\n",
    "max_item = max(items_ids) + 1\n",
    "print(\"n items:\", n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ids with most watched movies:  [655, 405, 450, 537, 416, 846, 682, 13, 880, 429, 896, 592, 796, 758, 561, 435, 551, 804, 474, 889] . . .\n",
      "\n",
      "Num cold start users:  200\n",
      "\n",
      "Drift user ids:  [463, 763, 621, 878, 758, 806, 913, 934, 504, 843, 825, 276, 654, 207, 484, 665, 864, 130, 455, 896, 862, 334, 899, 407, 622, 445, 773, 474, 268, 537, 882, 715, 749, 795, 194, 496, 13, 521, 774, 541, 339, 892, 497, 782, 536, 545, 291, 601, 815, 514, 886, 927, 660, 354, 385, 712, 399, 456, 303, 398, 721, 807, 907, 664, 608, 943, 642, 707, 903, 495, 305, 567, 85, 450, 682, 593, 757, 894, 346, 645, 7, 548, 897, 327, 671, 524, 629, 201, 666, 454, 222, 378, 901, 279, 374, 59, 840, 870, 271, 488, 435, 648, 561, 627, 727, 234, 922, 62, 711, 624, 606, 18, 851, 846, 788, 848, 532, 592, 699, 639, 299, 655, 1, 447, 405, 551, 751, 109, 328, 151, 178, 693, 503, 416, 883, 379, 738, 487, 618, 308, 881, 790, 486, 342, 577, 661, 458, 468, 586, 766, 373, 663, 452, 311, 453, 716, 347, 90, 698, 363, 919, 542, 269, 709, 94, 425, 479, 409, 92, 184, 543, 796, 889, 880, 406, 506, 437, 313, 643, 343, 145, 442, 932, 181, 805, 280, 389, 566, 650, 393, 833, 936, 345, 472, 387, 588, 498, 493, 286, 429]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# sanity check vs datapoints\n",
    "user_item_cnts = {}\n",
    "for user, movies in user_items.items():\n",
    "    user_item_cnts[user] = len(movies)\n",
    "\n",
    "top_users = dict(sorted(user_item_cnts.items(), key=operator.itemgetter(1), reverse=True)[:227])\n",
    "print(\"User_ids with most watched movies: \", list(top_users.keys())[0:20], \". . .\")\n",
    "\n",
    "cold_start_users = random.sample(list(top_users.keys()), k=200) # Select k users randomly\n",
    "print(\"\\nNum cold start users: \", len(cold_start_users))\n",
    "\n",
    "drift_users = random.sample(list(top_users.keys()), k=200)\n",
    "print(\"\\nDrift user ids: \", drift_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def movie_genres(movie_ids):\n",
    "    file = \"data/ml-100k/u.item\" \n",
    "    movies = pd.read_csv(file, sep='|', header=None, encoding='UTF-8')\n",
    "    drop_cols = [2, 3, 4]\n",
    "    movies = movies.drop(drop_cols, axis=1)\n",
    "    col_names = [\"movie_id\", \"title\", \"Unk\", \"Action\", \"Adventure\", \"Animation\",\n",
    "                  \"Childrens\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "                  \"Fantasy\", \"Film_Noir\", \"Horror\", \"Musical\", \"Mystery\",\n",
    "                  \"Romance\", \"Sci_Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "    movies.columns=col_names\n",
    "    movies.head()\n",
    "    return movies[movies['movie_id'].isin(movie_ids)].drop('title', axis=1)\n",
    "\n",
    "def order_movies(userId, movie_split=60):\n",
    "    file = \"data/ml-100k/u1.base\"\n",
    "    with open(filePath, \"rt\") as dataPath:\n",
    "        raw_data = dataPath.read().splitlines()\n",
    "    datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "    datapoints = np.array([[row[0], row[1], row[3]] for row in datapoints if row[0] == userId])\n",
    "    sorted(datapoints, key = lambda x: int(x[2]))\n",
    "    movie_set1, movie_set2 = [], []\n",
    "    count = 0\n",
    "    for row in datapoints:\n",
    "        count += 1\n",
    "        if count <= movie_split:\n",
    "            movie_set1.append(row[1])\n",
    "        elif movie_split < count <= movie_split*2:\n",
    "            movie_set2.append(row[1])\n",
    "        else:\n",
    "            break\n",
    "    if len(movie_set1) != len(movie_set2):\n",
    "        print(\"Error: Movie set lengths differ!\")\n",
    "        return -1, -1\n",
    "    return movie_set1, movie_set2\n",
    "\n",
    "def genre_distribution(genre_matrix):\n",
    "    col_sums = np.sum(genre_matrix.drop('movie_id', axis=1), axis=0)\n",
    "    distribution = col_sums/sum(col_sums)\n",
    "    return distribution\n",
    "\n",
    "def getCosineSimilarity(userId, movie_split):\n",
    "    # Get list of viewed movies ordered by timestamp\n",
    "    movie_set1, movie_set2 = order_movies(userId, movie_split)\n",
    "    # Get genre information for sets of movies\n",
    "    genre_matrix1 = movie_genres(movie_set1)\n",
    "    genre_matrix2 = movie_genres(movie_set2)\n",
    "    # Get genre distribution vectors set1 vs set2\n",
    "    dist1 = genre_distribution(genre_matrix1)\n",
    "    dist2 = genre_distribution(genre_matrix2)\n",
    "    # Calculate cosine similarity between vectors\n",
    "    similarity = cosine_similarity(np.array(dist1).reshape(1,-1), np.array(dist2).reshape(1,-1))\n",
    "    return similarity[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8187c5e228b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmovie_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrift_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcosim_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetCosineSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Slow since repeat file reads - fix if not lazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5362dc4b3b65>\u001b[0m in \u001b[0;36mgetCosineSimilarity\u001b[0;34m(userId, movie_split)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmovie_set1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_set2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder_movies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Get genre information for sets of movies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mgenre_matrix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_set1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mgenre_matrix2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_set2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Get genre distribution vectors set1 vs set2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5362dc4b3b65>\u001b[0m in \u001b[0;36mmovie_genres\u001b[0;34m(movie_ids)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/ml-100k/u.item\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdrop_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "cosim_dict = {}\n",
    "movie_split = 60\n",
    "for user in drift_users:\n",
    "    cosim_dict[user] = getCosineSimilarity(user, movie_split) # Slow since repeat file reads - fix if not lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cosim_ids = sorted(cosim_dict, key=cosim_dict.get)[0:5]\n",
    "lowest_cosim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # subtract max value to prevent overflow\\n\"\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "\n",
    "# computes average and standard deviation MRR for samples\n",
    "def mrr(R, U, V, samples):\n",
    "    mrrs = []\n",
    "    #print(R[samples, :][0])\n",
    "    for s in samples:\n",
    "        mrr = 0.0\n",
    "        rating_count = np.sum(R[s, :] >= 3.0)\n",
    "        if rating_count == 0:\n",
    "            continue\n",
    "        pred = np.matmul(U[s], V.T)\n",
    "        predRank = np.argsort(np.argsort(-pred)) + 1\n",
    "        rs, cs = R[s, :].nonzero()\n",
    "        for j in cs:\n",
    "            mrr += (1.0 / predRank[j])\n",
    "        mrr /= rating_count\n",
    "        mrrs.append(mrr)\n",
    "    return np.mean(mrrs), np.std(mrrs)\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (np.random.random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ONLY RUN IF TESTING ON DRIFT USERS ONLY \"\"\"\n",
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "drift_users = set(lowest_cosim_ids)\n",
    "datapoints = np.array([[row[0], row[1], row[2], row[3]] for row in datapoints if row[0] in drift_users])\n",
    "# timestamp sorted. So algo will go throw movies as if seeing user actions sequentially\n",
    "datapoints = sorted(datapoints, key = lambda x: int(x[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 2 \n",
    "k = 5 \n",
    "var = 0.5\n",
    "particles = [(1 / n_particles, {\"u\": np.random.normal(size=(max_user, k)),\n",
    "                                \"v\": np.random.normal(size=(max_item, k)),\n",
    "                                \"var_u\": 1.0,\n",
    "                                \"var_i\": 1.0}) for _ in range(n_particles)]\n",
    "\n",
    "# get mean rating to make rating data centered at 0\n",
    "#print(datapoints[:, 2])\n",
    "#mean_rating = np.mean(datapoints[:, 2])\n",
    "#data_store = {u_id: {row[1]: row[2] - mean_rating for row in datapoints if row[0] == u_id} for u_id in user_ids}\n",
    "\n",
    "\n",
    "data_store = {u_id: {row[1]: row[2] for row in datapoints if row[0] == u_id} for u_id in drift_users}\n",
    "\n",
    "user_history = {} # user_rating_history[user_id][\"item_ids\"], user_rating_history[user_id][\"ratings\"]\n",
    "item_history = {} # item_rating_history[item_id][\"user_ids\"], item_rating_history[item_id][\"ratings\"]\n",
    "ses = []\n",
    "ctr = 0\n",
    "ctr_hist = []\n",
    "#U_trace_hist =  [] #[ np.linalg.norm(particles[0][1]['u'],'fro') ]\n",
    "#V_trace_hist = [] # [ np.linalg.norm(particles[0][1]['v'],'fro') ]\n",
    "# initialize drift test\n",
    "drift1 = {}\n",
    "drift2 = {}\n",
    "rand1 = {}\n",
    "rand2 = {}\n",
    "user_cnt = {}\n",
    "for u_id in data_store.keys():\n",
    "    print(u_id)\n",
    "    drift1[u_id] = []\n",
    "    drift2[u_id] = []\n",
    "    user_cnt[u_id] = 0\n",
    "    rand1[u_id] = []\n",
    "    rand2[u_id] = []\n",
    "\n",
    "# what we're going to do here is assume that OUR system made the item recommendation and is observing the\n",
    "# rating we have in the dataset ... it's sort of like we're starting at line 11\n",
    "for _idx in range(len(datapoints)):\n",
    "\n",
    "    # randomly get a user\n",
    "    user_id = np.random.choice([i for i in data_store.keys()])\n",
    "    user_items = [i for i in data_store[user_id].keys()]\n",
    "    \n",
    "    # highest rating this user has\n",
    "    highest_rating = max(data_store[user_id].values())\n",
    "    # get highest rated items\n",
    "    highest_rated_items = [x for x in data_store[user_id].keys() if data_store[user_id][x] >= highest_rating]\n",
    "    # get indices for items this user rated\n",
    "    indices = np.array(user_items)\n",
    "    \n",
    "    # randomly select a particle\n",
    "    random_particle = np.random.choice(range(n_particles))\n",
    "    particle = particles[random_particle]\n",
    "    \n",
    "    # predict a rating only for the items rated by that user\n",
    "    predicted_rating = np.dot(particle[1][\"u\"][user_id, :], particle[1][\"v\"][indices, :].T)\n",
    "\n",
    "    # get the item id\n",
    "    max_rating_ind = np.argmax(predicted_rating)\n",
    "    item_id = [i for i in data_store[user_id].keys()][max_rating_ind]\n",
    "    \n",
    "\n",
    "    # add to ctr if possible\n",
    "    if item_id in highest_rated_items:\n",
    "        ctr += 1\n",
    "    ctr_hist.append(ctr / (_idx + 1))\n",
    "    \n",
    "    # get the true rating\n",
    "    rating = data_store[user_id][item_id]\n",
    "    \n",
    "    # For drift_users - track performance\n",
    "    user_cnt[user_id] += 1\n",
    "    if item_id in highest_rated_items and rating >= 3:\n",
    "        if user_cnt[user_id] <= 60:\n",
    "            drift1[user_id].append(1)\n",
    "        elif 60 < user_cnt[user_id] <= 120:\n",
    "            drift2[user_id].append(1)\n",
    "    else:\n",
    "        if user_cnt[user_id] <= 60:\n",
    "            drift1[user_id].append(0)\n",
    "        elif 60 < user_cnt[user_id] <= 120:\n",
    "            drift2[user_id].append(0)\n",
    "            \n",
    "    # Random recommendation baseline\n",
    "    rand_id = np.random.choice([i for i in data_store[user_id].keys()])\n",
    "    if rand_id in highest_rated_items and rating > 3:\n",
    "        if user_cnt[user_id] <= 60:\n",
    "            rand1[user_id].append(1)\n",
    "        elif 60 < user_cnt[user_id] <= 120:\n",
    "            rand2[user_id].append(1)\n",
    "    else:\n",
    "        if user_cnt[user_id] <= 60:\n",
    "            rand1[user_id].append(0)\n",
    "        elif 60 < user_cnt[user_id] <= 120:\n",
    "            rand2[user_id].append(0)\n",
    "    \n",
    "    # delete this item from this user\n",
    "    del data_store[user_id][item_id]\n",
    "    \n",
    "    # delete the user from the data store if they have no reviews left\n",
    "    if not data_store[user_id]:\n",
    "        del data_store[user_id]\n",
    "        \n",
    "    error = predicted_rating[max_rating_ind] - rating\n",
    "    se = error ** 2\n",
    "    if _idx % 101 == 0:\n",
    "        ses.append(se)\n",
    "    if _idx % 1000== 0:\n",
    "        print(\"squared error: {:.2f}\".format(se))\n",
    "\n",
    "    # line 17\n",
    "    precision_u_i = []\n",
    "    eta_u_i = []\n",
    "    for particle in particles:\n",
    "        if user_id not in user_history:\n",
    "            precision_u_i.append(np.eye(k))\n",
    "            eta_u_i.append(np.zeros(k))\n",
    "        else:\n",
    "            v_j = particle[1][\"v\"][user_history[user_id][\"item_ids\"], :]\n",
    "            lambda_u_i = 1 / var * np.dot(v_j.T, v_j) + 1 / particle[1][\"var_u\"] * np.eye(k)\n",
    "\n",
    "            precision_u_i.append(lambda_u_i)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    v_j,\n",
    "                    np.array(user_history[user_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "            eta_u_i.append(eta.reshape(-1))\n",
    "\n",
    "    # line 18\n",
    "    weights = []\n",
    "    mus = [1 / var * np.dot(np.linalg.inv(lambda_), eta) for lambda_, eta in zip(precision_u_i, eta_u_i)]\n",
    "    for particle, mu, precision in zip(particles, mus, precision_u_i):\n",
    "        v_j = particle[1][\"v\"][item_id, :]\n",
    "        cov = 1 / var + np.dot(np.dot(v_j.T, precision), v_j)\n",
    "        w = np.random.normal(\n",
    "            np.dot(v_j.T, mu),\n",
    "            cov\n",
    "        )\n",
    "        weights.append(w)\n",
    "    normalized_weights = softmax(weights)\n",
    "    \n",
    "    # Generate trace plots of U, V\n",
    "#     U_trace, V_trace = 0, 0\n",
    "#     for particle, weight in zip(particles, normalized_weights):\n",
    "#         U_trace += particle[1][\"u\"] * weight\n",
    "#         V_trace += particle[1][\"v\"] * weight\n",
    "#     U_trace_hist.append(np.linalg.norm(U_trace, 'fro'))\n",
    "#     V_trace_hist.append(np.linalg.norm(V_trace, 'fro'))\n",
    "\n",
    "    # line 19\n",
    "    ds = [np.random.choice(range(n_particles), p=normalized_weights) for _ in range(n_particles)]\n",
    "    p_prime = [{\"u\": np.copy(particles[d][1][\"u\"]),\n",
    "                \"v\": np.copy(particles[d][1][\"v\"]),\n",
    "                \"var_u\": particles[d][1][\"var_u\"],\n",
    "                \"var_i\": particles[d][1][\"var_i\"]} for d in ds]\n",
    "    for idx, (particle, precision, e) in enumerate(zip(p_prime, precision_u_i, eta_u_i)):\n",
    "\n",
    "        # line 21\n",
    "        v_j = particle[\"v\"][item_id, :]\n",
    "        add_to_precision = 1 / var * np.dot(v_j.reshape(-1, 1), v_j.reshape(1, -1))\n",
    "        precision += add_to_precision\n",
    "\n",
    "        add_to_eta = rating * v_j\n",
    "        e += add_to_eta\n",
    "\n",
    "        # line 22\n",
    "        p_prime[idx][\"u\"][user_id, :] = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision), e),\n",
    "            np.linalg.inv(precision)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # line 24\n",
    "        if item_id not in item_history:\n",
    "            precision_v_i = np.eye(k)\n",
    "            eta = np.zeros(k)\n",
    "        else:\n",
    "            u_i = particle[\"u\"][item_history[item_id][\"user_ids\"], :]\n",
    "            precision_v_i = 1 / var * \\\n",
    "                np.dot(u_i.T, u_i) + \\\n",
    "                1 / particle[\"var_i\"] * np.eye(k)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    u_i,\n",
    "                    np.array(item_history[item_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "        # line 25\n",
    "        p_prime[idx][\"v\"][item_id, :] = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision_v_i), eta),\n",
    "            np.linalg.inv(precision_v_i)\n",
    "        )\n",
    "\n",
    "    # line 28\n",
    "    particles = [(1 / n_particles, particle) for particle in p_prime]\n",
    "\n",
    "    if user_id not in user_history:\n",
    "        user_history[user_id] = {\"item_ids\": [], \"ratings\": []}\n",
    "    if item_id not in item_history:\n",
    "        item_history[item_id] = {\"user_ids\": [], \"ratings\": []}\n",
    "    user_history[user_id][\"item_ids\"].append(item_id)\n",
    "    user_history[user_id][\"ratings\"].append(rating)\n",
    "    item_history[item_id][\"user_ids\"].append(user_id)\n",
    "    item_history[item_id][\"ratings\"].append(rating)\n",
    "\n",
    "np.save(\"results/train_Particles={}_k={}_var={}.npy\".format(n_particles, k, var), particles)\n",
    "np.save(\"results/train_MSE_hist_p={}_k={}_var={}.npy\".format(n_particles, k, var), ses)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in drift1.keys():\n",
    "    rand_sum = sum(rand1[user]) + sum(rand2[user])\n",
    "    drift_sum = sum(drift1[user]) + sum(drift2[user])\n",
    "    #print(\"random sum: {} | drift sum: {}\".format(rand_sum, drift_sum))\n",
    "    print(\"user: \", user, \"  drift1:\", sum(drift1[user]), \"  rand1:\", sum(rand1[user]))\n",
    "    print(\"user: \", user, \"  drift2:\", sum(drift2[user]), \"  rand2:\", sum(rand2[user]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_rec = user_history[901]['item_ids']\n",
    "movie_set1 = movies_rec[0:60]\n",
    "movie_set2 = movies_rec[61:121]\n",
    "print(len(movie_set1) == len(movie_set2))\n",
    "\n",
    "genre_matrix1 = movie_genres(movie_set1)\n",
    "genre_matrix2 = movie_genres(movie_set2)\n",
    "dist1 = genre_distribution(genre_matrix1)\n",
    "dist2 = genre_distribution(genre_matrix2)\n",
    "similarity = cosine_similarity(np.array(dist1).reshape(1,-1), np.array(dist2).reshape(1,-1))[0][0]\n",
    "\n",
    "print(dist1)\n",
    "print()\n",
    "print(dist2)\n",
    "print()\n",
    "print(dist1-dist2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(U_trace_hist)), U_trace_hist)\n",
    "plt.title(\"$\\|\\|U\\|\\|_{Fro}^2$ at Each Sample\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.savefig(\"weird_U_norm_trace.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.plot(range(len(V_trace_hist)), V_trace_hist)\n",
    "# plt.title(\"$\\|V\\|_{Fro}^2$ at Each Sample\")\n",
    "# plt.xlabel(\"Time Step\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def test(particles, data):\n",
    "    \n",
    "    user_ids = set([item[0] for item in data])\n",
    "    items_ids = set([item[1] for item in data])\n",
    "    data_store = {u_id: {row[1]: row[2] for row in data if row[0] == u_id} for u_id in user_ids}\n",
    "    user_history = {} # user_rating_history[user_id][\"item_ids\"], user_rating_history[user_id][\"ratings\"]\n",
    "    item_history = {} # item_rating_history[item_id][\"user_ids\"], item_rating_history[item_id][\"ratings\"]\n",
    "\n",
    "    #Evaluation stuffs\n",
    "    precision = 0\n",
    "    ses = []\n",
    "    precision_list = []\n",
    "    \n",
    "    for _idx in range(len(data)):\n",
    "        \n",
    "        # randomly get a user\n",
    "        user_id = np.random.choice([i for i in data_store.keys()])\n",
    "        user_items = [i for i in data_store[user_id].keys()]\n",
    "        \n",
    "        # get indices for items this user rated\n",
    "        indices = np.array(user_items)\n",
    "        \n",
    "        # randomly select a particle\n",
    "        random_particle = np.random.choice(range(len(particles)))\n",
    "        particle = particles[random_particle]\n",
    "        \n",
    "        # predict a rating only for the items rated by that user\n",
    "        predicted_rating = np.dot( particle[1][\"u\"][user_id, :], particle[1][\"v\"][indices, :].T)\n",
    "        \n",
    "        # get the item id\n",
    "        max_rating_ind = np.argmax(predicted_rating)\n",
    "        item_id = [i for i in data_store[user_id].keys()][max_rating_ind]\n",
    "        \n",
    "        # get the true rating\n",
    "        rating = data_store[user_id][item_id]\n",
    "\n",
    "        # delete this item from this user\n",
    "        del data_store[user_id][item_id]\n",
    "\n",
    "        # delete the user from the data store if they have no reviews left\n",
    "        if not data_store[user_id]:\n",
    "            del data_store[user_id]\n",
    "          \n",
    "\n",
    "        #pred = predicted_rating[max_rating_ind]\n",
    "        pred = random.randint(1,5)\n",
    "\n",
    "        if pred > 5:\n",
    "            pred = 5\n",
    "        elif pred < 1:\n",
    "            pred = 1\n",
    "    \n",
    "        error = pred - rating\n",
    "        se = error ** 2\n",
    "        \n",
    "        # Precision\n",
    "        if pred >= 4 and rating >= 4:\n",
    "            precision += 1\n",
    "        if _idx % 10 == 0:\n",
    "            ses.append(se)\n",
    "        if _idx % 100 == 0:\n",
    "            print(\"squared error: {:.2f}\".format(se))\n",
    "        if _idx in [10, 20, 40 ,60, 80, 120]:\n",
    "                precision_list.append((_idx, precision))\n",
    "        \n",
    "        # line 17\n",
    "        precision_u_i = []\n",
    "        eta_u_i = []\n",
    "        for particle in particles:\n",
    "            if user_id not in user_history:\n",
    "                precision_u_i.append(np.eye(k))\n",
    "                eta_u_i.append(np.zeros(k))\n",
    "            else:\n",
    "                v_j = particle[1][\"v\"][user_history[user_id][\"item_ids\"], :]\n",
    "                lambda_u_i = 1 / var * np.dot(v_j.T, v_j) + 1 / particle[1][\"var_u\"] * np.eye(k)\n",
    "\n",
    "                precision_u_i.append(lambda_u_i)\n",
    "\n",
    "                eta = np.sum(\n",
    "                    np.multiply(\n",
    "                        v_j,\n",
    "                        np.array(user_history[user_id][\"ratings\"]).reshape(-1, 1)\n",
    "                    ),\n",
    "                    axis=0\n",
    "                )\n",
    "                eta_u_i.append(eta.reshape(-1))\n",
    "\n",
    "        # line 18\n",
    "        weights = []\n",
    "        mus = [1 / var * np.dot(np.linalg.inv(lambda_), eta) for lambda_, eta in zip(precision_u_i, eta_u_i)]\n",
    "        for particle, mu, precision in zip(particles, mus, precision_u_i):\n",
    "            v_j = particle[1][\"v\"][item_id, :]\n",
    "            cov = 1 / var + np.dot(np.dot(v_j.T, precision), v_j)\n",
    "            w = np.random.normal(\n",
    "                np.dot(v_j.T, mu),\n",
    "                cov\n",
    "            )\n",
    "            weights.append(w)\n",
    "        normalized_weights = softmax(weights)\n",
    "\n",
    "        # line 19\n",
    "        ds = [np.random.choice(range(n_particles), p=normalized_weights) for _ in range(n_particles)]\n",
    "        p_prime = []\n",
    "        for d in ds:\n",
    "            p_prime.append((1 / n_particles, particles[d][1]))\n",
    "\n",
    "        for particle, precision, e in zip(p_prime, precision_u_i, eta_u_i):\n",
    "\n",
    "            # line 21\n",
    "            v_j = particle[1][\"v\"][item_id, :]\n",
    "            add_to_precision = 1 / var * np.dot(v_j.reshape(-1, 1), v_j.reshape(1, -1))\n",
    "            precision += add_to_precision\n",
    "\n",
    "            add_to_eta = rating * v_j\n",
    "            e += add_to_eta\n",
    "\n",
    "            # line 22\n",
    "            particle[1][\"u\"][user_id, :] = np.random.multivariate_normal(\n",
    "                1 / var * np.dot(np.linalg.inv(precision), e),\n",
    "                np.linalg.inv(precision)\n",
    "            )\n",
    "\n",
    "            # line 24\n",
    "            if item_id not in item_history:\n",
    "                precision_v_i = np.eye(k)\n",
    "                eta = np.zeros(k)\n",
    "            else:\n",
    "                u_i = particle[1][\"u\"][item_history[item_id][\"user_ids\"], :]\n",
    "                precision_v_i = 1 / var * \\\n",
    "                    np.dot(u_i.T, u_i) + \\\n",
    "                    1 / particle[1][\"var_i\"] * np.eye(k)\n",
    "\n",
    "                eta = np.sum(\n",
    "                    np.multiply(\n",
    "                        u_i,\n",
    "                        np.array(item_history[item_id][\"ratings\"]).reshape(-1, 1)\n",
    "                    ),\n",
    "                    axis=0\n",
    "                )\n",
    "            # line 25\n",
    "            particle[1][\"v\"][item_id, :] = np.random.multivariate_normal(\n",
    "                1 / var * np.dot(np.linalg.inv(precision_v_i), eta),\n",
    "                np.linalg.inv(precision_v_i)\n",
    "            )\n",
    "\n",
    "        # line 28\n",
    "        particles = p_prime\n",
    "\n",
    "        if user_id not in user_history:\n",
    "            user_history[user_id] = {\"item_ids\": [], \"ratings\": []}\n",
    "        if item_id not in item_history:\n",
    "            item_history[item_id] = {\"user_ids\": [], \"ratings\": []}\n",
    "        user_history[user_id][\"item_ids\"].append(item_id)\n",
    "        user_history[user_id][\"ratings\"].append(rating)\n",
    "        item_history[item_id][\"user_ids\"].append(user_id)\n",
    "        item_history[item_id][\"ratings\"].append(rating)\n",
    "    return ses, precision_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Cold Start Test \"\"\"\n",
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "    \n",
    "data = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "data = np.array([[row[0], row[1], row[2], row[3]] for row in data if row[0] in cold_start_users])\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "test_ses, ctr_list = test(particles, data)\n",
    "\n",
    "#np.save(\"results/test_MSE_hist_p={}_k={}_var={}_run2.npy\".format(n_particles, k, var), test_ses)   \n",
    "# print()\n",
    "for idx, ctr in ctr_list:\n",
    "    print(\"idx: {}, ctr: {}\".format(idx, ctr/len(cold_start_users)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"data/ml-100k/u1.test\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "    \n",
    "data = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "data = np.array([[row[0], row[1], row[2], row[3]] for row in data if row[0] in cold_start_users])\n",
    "#data = np.array([[row[0], row[1], row[2], row[3]] for row in data])\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "test_ses, ctr_list = test(particles, data)\n",
    "\n",
    "#np.save(\"results/test_MSE_hist_p={}_k={}_var={}_run2.npy\".format(n_particles, k, var), test_ses)   \n",
    "print()\n",
    "for idx, ctr in ctr_list:\n",
    "    print(\"idx: {}, ctr: {}\".format(idx, ctr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x):\n",
    "    avgs = []\n",
    "    for i, v in enumerate(x):\n",
    "        avgs.append(np.sum(x[:i]) / i)\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(test_ses)\n",
    "print(mses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.array([2.44121342564, 2.40624590454, 2.29556823627, 2.30130174491, 2.19908182858])\n",
    "\n",
    "avg = np.average(m)\n",
    "std = np.std(m)\n",
    "min_v = np.min(m)\n",
    "max_v = np.max(m)\n",
    "print(\"avg:\", avg)\n",
    "print(\"std:\", std)\n",
    "print(\"min:\", min_v)\n",
    "print(\"max:\", max_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(ses)\n",
    "plt.plot(range(len(mses)), mses)\n",
    "plt.title(\"Train MSE: {:.2f} | Particles=2 K=5\".format(mses[-1]))\n",
    "plt.savefig(\"MSE_graph.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 user - get u vec \n",
    "# know how \n",
    "w = [.15, .25, .60 ]\n",
    "l = [1, 2, 3]\n",
    "\n",
    "x = np.random.random(l, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(ctr_hist)), ctr_hist)\n",
    "plt.title(\"cumulative take rate {:.2f}\".format(ctr_hist[-1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
