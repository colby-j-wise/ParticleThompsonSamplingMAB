{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n users: 943\n",
      "n items: 1650\n"
     ]
    }
   ],
   "source": [
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "datapoints = np.array([[row[0], row[1], row[2], row[3]] for row in datapoints])\n",
    "np.random.shuffle(datapoints)\n",
    "\n",
    "# Used in precision eval... not rly needed given datapoints\n",
    "user_items = defaultdict(list)\n",
    "for row in datapoints:\n",
    "    user_items[row[0]].append(row[1])\n",
    "\n",
    "user_ids = set([datapoint[0] for datapoint in datapoints])\n",
    "n_users = len(user_ids)\n",
    "max_user = max(user_ids) + 1\n",
    "print(\"n users:\", n_users)\n",
    "items_ids = set([datapoint[1] for datapoint in datapoints])\n",
    "n_items = len(items_ids)\n",
    "max_item = max(items_ids) + 1\n",
    "print(\"n items:\", n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_ids with most watched movies:  [655, 405, 450, 537, 416, 846, 682, 13, 880, 429, 896, 592, 796, 758, 561, 435, 551, 804, 474, 889] . . .\n",
      "\n",
      "Num cold start users:  200\n",
      "\n",
      "Drift user ids:  [798, 934, 566, 311, 85, 654, 305, 13, 882, 409, 447, 394, 457, 815, 527, 586, 279, 901, 766, 721, 707, 429, 551, 846, 795, 608, 59, 145, 18, 385, 823, 671, 709, 561, 417, 303, 498, 847, 932, 848, 749, 435, 532, 694, 919, 665, 524, 883, 899, 715, 493, 504, 453, 468, 363, 548, 334, 445, 889, 757, 506, 773, 642, 897, 751, 567, 542, 533, 269, 543, 276, 864, 399, 455, 593, 151, 833, 840, 109, 374, 234, 346, 474, 327, 201, 342, 660, 312, 634, 514, 379, 927, 537, 328, 393, 313, 894, 398, 1, 738, 782, 913, 314, 643, 472, 601, 194, 727, 645, 804, 666, 487, 373, 536, 545, 301, 463, 354, 653, 184, 347, 682, 870, 271, 293, 95, 268, 343, 663, 207, 389, 442, 698, 907, 862, 747, 711, 943, 699, 629, 178, 664, 933, 650, 618, 878, 622, 458, 222, 592, 387, 436, 825, 62, 535, 130, 805, 7, 763, 479, 627, 92, 345, 405, 892, 624, 806, 425, 486, 903, 577, 456, 280, 488, 500, 407, 503, 916, 496, 922, 790, 796, 90, 655, 843, 788, 868, 621, 880, 452, 758, 716, 495, 881, 648, 886, 896, 497, 291, 541]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# sanity check vs datapoints\n",
    "user_item_cnts = {}\n",
    "for user, movies in user_items.items():\n",
    "    user_item_cnts[user] = len(movies)\n",
    "\n",
    "top_users = dict(sorted(user_item_cnts.items(), key=operator.itemgetter(1), reverse=True)[:227])\n",
    "print(\"User_ids with most watched movies: \", list(top_users.keys())[0:20], \". . .\")\n",
    "\n",
    "cold_start_users = random.sample(list(top_users.keys()), k=200) # Select k users randomly\n",
    "print(\"\\nNum cold start users: \", len(cold_start_users))\n",
    "\n",
    "drift_users = random.sample(list(top_users.keys()), k=200)\n",
    "print(\"\\nDrift user ids: \", drift_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def movie_genres(movie_ids):\n",
    "    file = \"data/ml-100k/u.item\" \n",
    "    movies = pd.read_csv(file, sep='|', header=None, encoding='UTF-8')\n",
    "    drop_cols = [2, 3, 4]\n",
    "    movies = movies.drop(drop_cols, axis=1)\n",
    "    col_names = [\"movie_id\", \"title\", \"Unk\", \"Action\", \"Adventure\", \"Animation\",\n",
    "                  \"Childrens\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "                  \"Fantasy\", \"Film_Noir\", \"Horror\", \"Musical\", \"Mystery\",\n",
    "                  \"Romance\", \"Sci_Fi\", \"Thriller\", \"War\", \"Western\"]\n",
    "    movies.columns=col_names\n",
    "    movies.head()\n",
    "    return movies[movies['movie_id'].isin(movie_ids)].drop('title', axis=1)\n",
    "\n",
    "def order_movies(userId, movie_split=60):\n",
    "    file = \"data/ml-100k/u1.base\"\n",
    "    with open(filePath, \"rt\") as dataPath:\n",
    "        raw_data = dataPath.read().splitlines()\n",
    "    datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "    datapoints = np.array([[row[0], row[1], row[3]] for row in datapoints if row[0] == userId])\n",
    "    sorted(datapoints, key = lambda x: int(x[2]))\n",
    "    movie_set1, movie_set2 = [], []\n",
    "    count = 0\n",
    "    for row in datapoints:\n",
    "        count += 1\n",
    "        if count <= movie_split:\n",
    "            movie_set1.append(row[1])\n",
    "        elif movie_split < count <= movie_split*2:\n",
    "            movie_set2.append(row[1])\n",
    "        else:\n",
    "            break\n",
    "    if len(movie_set1) != len(movie_set2):\n",
    "        print(\"Error: Movie set lengths differ!\")\n",
    "        return -1, -1\n",
    "    return movie_set1, movie_set2\n",
    "\n",
    "def genre_distribution(genre_matrix):\n",
    "    col_sums = np.sum(genre_matrix.drop('movie_id', axis=1), axis=0)\n",
    "    distribution = col_sums/sum(col_sums)\n",
    "    return distribution\n",
    "\n",
    "def getCosineSimilarity(userId, movie_split):\n",
    "    # Get list of viewed movies ordered by timestamp\n",
    "    movie_set1, movie_set2 = order_movies(userId, movie_split)\n",
    "    # Get genre information for sets of movies\n",
    "    genre_matrix1 = movie_genres(movie_set1)\n",
    "    genre_matrix2 = movie_genres(movie_set2)\n",
    "    # Get genre distribution vectors set1 vs set2\n",
    "    dist1 = genre_distribution(genre_matrix1)\n",
    "    dist2 = genre_distribution(genre_matrix2)\n",
    "    # Calculate cosine similarity between vectors\n",
    "    similarity = cosine_similarity(np.array(dist1).reshape(1,-1), np.array(dist2).reshape(1,-1))\n",
    "    return similarity[0][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8187c5e228b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmovie_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdrift_users\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcosim_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetCosineSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_split\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Slow since repeat file reads - fix if not lazy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5362dc4b3b65>\u001b[0m in \u001b[0;36mgetCosineSimilarity\u001b[0;34m(userId, movie_split)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmovie_set1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_set2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder_movies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovie_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Get genre information for sets of movies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mgenre_matrix1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_set1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mgenre_matrix2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_set2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Get genre distribution vectors set1 vs set2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5362dc4b3b65>\u001b[0m in \u001b[0;36mmovie_genres\u001b[0;34m(movie_ids)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmovie_genres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data/ml-100k/u.item\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdrop_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "cosim_dict = {}\n",
    "movie_split = 60\n",
    "for user in drift_users:\n",
    "    cosim_dict[user] = getCosineSimilarity(user, movie_split) # Slow since repeat file reads - fix if not lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_cosim_ids = sorted(cosim_dict, key=cosim_dict.get)[0:5]\n",
    "lowest_cosim_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # subtract max value to prevent overflow\\n\"\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_particles = 2\n",
    "k = 2\n",
    "var = 0.5\n",
    "particles = [(1 / n_particles, {\"u\": np.random.normal(size=(max_user, k)),\n",
    "                                \"v\": np.random.normal(size=(max_item, k)),\n",
    "                                \"var_u\": 1.0,\n",
    "                                \"var_i\": 1.0}) for _ in range(n_particles)]\n",
    "\n",
    "# get mean rating to make rating data centered at 0\n",
    "print(datapoints[:, 2])\n",
    "mean_rating = np.mean(datapoints[:, 2])\n",
    "data_store = {u_id: {row[1]: row[2] - mean_rating for row in datapoints if row[0] == u_id} for u_id in user_ids}\n",
    "\n",
    "\n",
    "#data_store = {u_id: {row[1]: row[2] for row in datapoints if row[0] == u_id} for u_id in user_ids if u_id not in lowest_cosim_ids}\n",
    "\n",
    "user_history = {} # user_rating_history[user_id][\"item_ids\"], user_rating_history[user_id][\"ratings\"]\n",
    "item_history = {} # item_rating_history[item_id][\"user_ids\"], item_rating_history[item_id][\"ratings\"]\n",
    "ses = []\n",
    "ctr = 0\n",
    "ctr_hist = []\n",
    "\n",
    "# initialize drift test\n",
    "d1, d2, d3 = {}, {}, {}\n",
    "r1, r2, r3 = {}, {}, {}\n",
    "user_cnt = {}\n",
    "for u_id in lowest_cosim_ids:\n",
    "    d1[u_id], d2[u_id], d3[u_id] = [], [], []\n",
    "    r1[u_id], r2[u_id], r3[u_id] = [], [], []\n",
    "    user_cnt[u_id] = 0\n",
    "\n",
    "# what we're going to do here is assume that OUR system made the item recommendation and is observing the\n",
    "# rating we have in the dataset ... it's sort of like we're starting at line 11\n",
    "for _idx in range(len(datapoints)):\n",
    "\n",
    "    # randomly get a user\n",
    "    user_id = np.random.choice([i for i in data_store.keys()])\n",
    "    user_items = [i for i in data_store[user_id].keys()]\n",
    "    \n",
    "    # highest rating this user has\n",
    "    highest_rating = max(data_store[user_id].values())\n",
    "    # get highest rated items\n",
    "    highest_rated_items = [x for x in data_store[user_id].keys() if data_store[user_id][x] >= highest_rating]\n",
    "    # get indices for items this user rated\n",
    "    indices = np.array(user_items)\n",
    "    \n",
    "    # randomly select a particle\n",
    "    random_particle = np.random.choice(range(n_particles))\n",
    "    particle = particles[random_particle]\n",
    "    \n",
    "    # predict a rating only for the items rated by that user\n",
    "    predicted_rating = np.dot(particle[1][\"u\"][user_id, :], particle[1][\"v\"][indices, :].T)\n",
    "\n",
    "    # get the item id\n",
    "    max_rating_ind = np.argmax(predicted_rating)\n",
    "    item_id = [i for i in data_store[user_id].keys()][max_rating_ind]\n",
    "    # get rand item id for random baseline\n",
    "    rand_id = np.random.choice([i for i in data_store[user_id].keys()])\n",
    "\n",
    "    # add to ctr if possible\n",
    "    if item_id in highest_rated_items:\n",
    "        ctr += 1\n",
    "    ctr_hist.append(ctr / (_idx + 1))\n",
    "    \n",
    "    \n",
    "    # For drift_users - track performance\n",
    "    # Skeptical of this highest_rated_items thing\n",
    "    if user_id in lowest_cosim_ids:\n",
    "        user_cnt[user_id] += 1\n",
    "        if item_id in highest_rated_items:\n",
    "            if user_cnt[user_id] <= 60:\n",
    "                d1[user_id].append(1)\n",
    "            elif 60 < user_cnt[user_id] <= 120:\n",
    "                d2[user_id].append(1)\n",
    "            elif 120 < user_cnt[user_id] <= 180:\n",
    "                d3[user_id].append(1)\n",
    "        else:\n",
    "            if user_cnt[user_id] <= 60:\n",
    "                d1[user_id].append(0)\n",
    "            elif 60 < user_cnt[user_id] <= 120:\n",
    "                d2[user_id].append(0)\n",
    "            elif 120 < user_cnt[user_id] <= 180:\n",
    "                d3[user_id].append(0)\n",
    "\n",
    "        # Random recommendation baseline\n",
    "        if rand_id in highest_rated_items:\n",
    "            if user_cnt[user_id] <= 60:\n",
    "                r1[user_id].append(1)\n",
    "            elif 60 < user_cnt[user_id] <= 120:\n",
    "                r2[user_id].append(1)\n",
    "            elif 120 < user_cnt[user_id] <= 180:\n",
    "                r3[user_id].append(1)\n",
    "        else:\n",
    "            if user_cnt[user_id] <= 60:\n",
    "                r1[user_id].append(0)\n",
    "            elif 60 < user_cnt[user_id] <= 120:\n",
    "                r2[user_id].append(0)\n",
    "            elif 120 < user_cnt[user_id] <= 180:\n",
    "                r3[user_id].append(0)\n",
    "            \n",
    "    # get the true rating\n",
    "    rating = data_store[user_id][item_id]\n",
    "    \n",
    "    # delete this item from this user\n",
    "    del data_store[user_id][item_id]\n",
    "    \n",
    "    # delete the user from the data store if they have no reviews left\n",
    "    if not data_store[user_id]:\n",
    "        del data_store[user_id]\n",
    "        \n",
    "    error = predicted_rating[max_rating_ind] - rating\n",
    "    se = error ** 2\n",
    "    if _idx % 101 == 0:\n",
    "        ses.append(se)\n",
    "    if _idx % 1000== 0:\n",
    "        print(\"squared error: {:.2f}\".format(se))\n",
    "\n",
    "    # line 17\n",
    "    precision_u_i = []\n",
    "    eta_u_i = []\n",
    "    for particle in particles:\n",
    "        if user_id not in user_history:\n",
    "            precision_u_i.append(np.eye(k))\n",
    "            eta_u_i.append(np.zeros(k))\n",
    "        else:\n",
    "            v_j = particle[1][\"v\"][user_history[user_id][\"item_ids\"], :]\n",
    "            lambda_u_i = 1 / var * np.dot(v_j.T, v_j) + 1 / particle[1][\"var_u\"] * np.eye(k)\n",
    "\n",
    "            precision_u_i.append(lambda_u_i)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    v_j,\n",
    "                    np.array(user_history[user_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "            eta_u_i.append(eta.reshape(-1))\n",
    "\n",
    "    # line 18\n",
    "    weights = []\n",
    "    mus = [1 / var * np.dot(np.linalg.inv(lambda_), eta) for lambda_, eta in zip(precision_u_i, eta_u_i)]\n",
    "    for particle, mu, precision in zip(particles, mus, precision_u_i):\n",
    "        v_j = particle[1][\"v\"][item_id, :]\n",
    "        cov = 1 / var + np.dot(np.dot(v_j.T, precision), v_j)\n",
    "        w = np.random.normal(\n",
    "            np.dot(v_j.T, mu),\n",
    "            cov\n",
    "        )\n",
    "        weights.append(w)\n",
    "    normalized_weights = softmax(weights)\n",
    "    \n",
    "    # line 19\n",
    "    ds = [np.random.choice(range(n_particles), p=normalized_weights) for _ in range(n_particles)]\n",
    "    p_prime = [{\"u\": np.copy(particles[d][1][\"u\"]),\n",
    "                \"v\": np.copy(particles[d][1][\"v\"]),\n",
    "                \"var_u\": particles[d][1][\"var_u\"],\n",
    "                \"var_i\": particles[d][1][\"var_i\"]} for d in ds]\n",
    "    for idx, (particle, precision, e) in enumerate(zip(p_prime, precision_u_i, eta_u_i)):\n",
    "\n",
    "        # line 21\n",
    "        v_j = particle[\"v\"][item_id, :]\n",
    "        add_to_precision = 1 / var * np.dot(v_j.reshape(-1, 1), v_j.reshape(1, -1))\n",
    "        precision += add_to_precision\n",
    "\n",
    "        add_to_eta = rating * v_j\n",
    "        e += add_to_eta\n",
    "\n",
    "        # line 22\n",
    "        p_prime[idx][\"u\"][user_id, :] = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision), e),\n",
    "            np.linalg.inv(precision)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # line 24\n",
    "        if item_id not in item_history:\n",
    "            precision_v_i = np.eye(k)\n",
    "            eta = np.zeros(k)\n",
    "        else:\n",
    "            u_i = particle[\"u\"][item_history[item_id][\"user_ids\"], :]\n",
    "            precision_v_i = 1 / var * \\\n",
    "                np.dot(u_i.T, u_i) + \\\n",
    "                1 / particle[\"var_i\"] * np.eye(k)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    u_i,\n",
    "                    np.array(item_history[item_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "        # line 25\n",
    "        p_prime[idx][\"v\"][item_id, :] = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision_v_i), eta),\n",
    "            np.linalg.inv(precision_v_i)\n",
    "        )\n",
    "\n",
    "    # line 28\n",
    "    particles = [(1 / n_particles, particle) for particle in p_prime]\n",
    "\n",
    "    if user_id not in user_history:\n",
    "        user_history[user_id] = {\"item_ids\": [], \"ratings\": []}\n",
    "    if item_id not in item_history:\n",
    "        item_history[item_id] = {\"user_ids\": [], \"ratings\": []}\n",
    "    user_history[user_id][\"item_ids\"].append(item_id)\n",
    "    user_history[user_id][\"ratings\"].append(rating)\n",
    "    item_history[item_id][\"user_ids\"].append(user_id)\n",
    "    item_history[item_id][\"ratings\"].append(rating)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in d1.keys():\n",
    "    print(\"user: \", user, \"  drift1:\", sum(d1[user]), \"  rand1:\", sum(r1[user]))\n",
    "    print(\"user: \", user, \"  drift2:\", sum(d2[user]), \"  rand2:\", sum(r2[user]))\n",
    "    print(\"user: \", user, \"  drift3:\", sum(d3[user]), \"  rand3:\", sum(r3[user]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(userId, dist1, dist2):\n",
    "    fig = plt.figure();\n",
    "    ax = fig.add_subplot(111); # Create matplotlib axes\n",
    "    dist1.plot(kind='bar', color='pink', ax=ax, position=1)\n",
    "    dist2.plot(kind='bar', color='blue', ax=ax, position=0)\n",
    "    ax.set_ylabel('% Genre')\n",
    "    plt.title('User {} Genre Distrubtion'.format(userId))\n",
    "    plt.legend(['Before', 'After'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"users in test set: \", lowest_cosim_ids)\n",
    "user = lowest_cosim_ids[1]\n",
    "movies_rec = user_history[user]['item_ids']\n",
    "movie_set1 = movies_rec[0:60]\n",
    "movie_set2 = movies_rec[61:121]\n",
    "\n",
    "genre_matrix1 = movie_genres(movie_set1)\n",
    "genre_matrix2 = movie_genres(movie_set2)\n",
    "dist1 = genre_distribution(genre_matrix1)\n",
    "dist2 = genre_distribution(genre_matrix2)\n",
    "similarity = cosine_similarity(np.array(dist1).reshape(1,-1), np.array(dist2).reshape(1,-1))[0][0]\n",
    "\n",
    "print(\"\\nPrevious 60 Movie Genre Distribution \\n\", dist1)\n",
    "print()\n",
    "print(\"\\nNext 60 Movie Genre Distribution \\n\", dist2)\n",
    "print()\n",
    "print(\"\\n% Chnge per Genre \\n\", dist1-dist2)\n",
    "print(similarity)\n",
    "\n",
    "plot_histogram(user, dist1, dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([dist1, dist2], label=['dist1', 'dist2'])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x):\n",
    "    avgs = []\n",
    "    for i, v in enumerate(x):\n",
    "        avgs.append(np.sum(x[:i]) / i)\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(test_ses)\n",
    "print(mses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(ses)\n",
    "plt.plot(range(len(mses)), mses)\n",
    "plt.title(\"Train MSE: {:.2f} | Particles=2 K=5\".format(mses[-1]))\n",
    "plt.savefig(\"MSE_graph.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(ctr_hist)), ctr_hist)\n",
    "plt.title(\"cumulative take rate {:.2f}\".format(ctr_hist[-1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
