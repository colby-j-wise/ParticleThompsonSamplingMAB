{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n users: 943\n",
      "n items: 1650\n"
     ]
    }
   ],
   "source": [
    "filePath = \"data/ml-100k/u1.base\"\n",
    "with open(filePath, \"rt\") as dataPath:\n",
    "    raw_data = dataPath.read().splitlines()\n",
    "datapoints = [[int(i) for i in data.split(\"\\t\")] for data in raw_data]\n",
    "\n",
    "# indexing on users/movies starts at 1, reset to index from 0, this will be important when we do testing\n",
    "datapoints = np.array([[row[0], row[1], row[2], row[3]] for row in datapoints])\n",
    "np.random.shuffle(datapoints)\n",
    "\n",
    "user_ids = set([datapoint[0] for datapoint in datapoints])\n",
    "n_users = len(user_ids)\n",
    "max_user = max(user_ids) + 1\n",
    "print(\"n users:\", n_users)\n",
    "items_ids = set([datapoint[1] for datapoint in datapoints])\n",
    "n_items = len(items_ids)\n",
    "max_item = max(items_ids) + 1\n",
    "print(\"n items:\", n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # subtract max value to prevent overflow\\n\"\n",
    "    return np.exp(x - np.max(x)) / np.sum(np.exp(x - np.max(x)))\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (np.random.random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "def systematic_resample(weights):\n",
    "    N = len(weights)\n",
    "\n",
    "    # make N subdivisions, and choose positions with a consistent random offset\n",
    "    positions = (np.random.random() + np.arange(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared error: 0.60\n",
      "squared error: 19.91\n",
      "squared error: 5.13\n",
      "squared error: 16.37\n",
      "squared error: 13.64\n",
      "squared error: 3.69\n",
      "squared error: 1.23\n",
      "squared error: 0.02\n",
      "squared error: 0.93\n",
      "squared error: 9.37\n",
      "squared error: 0.93\n",
      "squared error: 4.18\n",
      "squared error: 0.43\n",
      "squared error: 0.97\n",
      "squared error: 2.94\n",
      "squared error: 1.87\n",
      "squared error: 0.00\n",
      "squared error: 4.09\n",
      "squared error: 0.00\n",
      "squared error: 0.02\n",
      "squared error: 4.09\n",
      "squared error: 0.01\n",
      "squared error: 2.47\n",
      "squared error: 3.25\n",
      "squared error: 0.16\n",
      "squared error: 0.00\n",
      "squared error: 0.33\n",
      "squared error: 0.72\n",
      "squared error: 0.00\n",
      "squared error: 1.35\n",
      "squared error: 0.21\n",
      "squared error: 12.62\n",
      "squared error: 0.05\n",
      "squared error: 0.26\n",
      "squared error: 0.10\n",
      "squared error: 0.00\n",
      "squared error: 0.88\n",
      "squared error: 10.50\n",
      "squared error: 0.08\n",
      "squared error: 0.16\n",
      "squared error: 0.01\n",
      "squared error: 0.02\n",
      "squared error: 0.03\n",
      "squared error: 1.81\n",
      "squared error: 0.01\n",
      "squared error: 0.00\n",
      "squared error: 0.00\n",
      "squared error: 0.19\n",
      "squared error: 0.72\n",
      "squared error: 0.83\n",
      "squared error: 0.46\n",
      "squared error: 1.86\n",
      "squared error: 0.10\n",
      "squared error: 0.05\n",
      "squared error: 0.94\n",
      "squared error: 1.96\n",
      "squared error: 0.93\n",
      "squared error: 3.29\n",
      "squared error: 2.56\n",
      "squared error: 1.10\n",
      "squared error: 2.35\n",
      "squared error: 0.01\n",
      "squared error: 0.15\n",
      "squared error: 1.93\n",
      "squared error: 0.02\n"
     ]
    }
   ],
   "source": [
    "n_particles = 5\n",
    "k = 2\n",
    "var = 0.5\n",
    "particles = [(1 / n_particles, {\"u\": np.random.normal(size=(max_user, k)),\n",
    "                                \"v\": np.random.normal(size=(max_item, k)),\n",
    "                                \"var_u\": 1.0,\n",
    "                                \"var_i\": 1.0}) for _ in range(n_particles)]\n",
    "\n",
    "# get mean rating to make rating data centered at 0\n",
    "mean_rating = np.mean(datapoints[:, 2])\n",
    "data_store = {u_id: {row[1]: row[2] - mean_rating for row in datapoints if row[0] == u_id} for u_id in user_ids}\n",
    "\n",
    "user_history = {} # user_rating_history[user_id][\"item_ids\"], user_rating_history[user_id][\"ratings\"]\n",
    "item_history = {} # item_rating_history[item_id][\"user_ids\"], item_rating_history[item_id][\"ratings\"]\n",
    "ses = []\n",
    "ctr = 0\n",
    "ctr_hist = []\n",
    "uniParticlesList = []\n",
    "multi, strat, system = [], [], []\n",
    "\n",
    "for _idx in range(len(datapoints)):\n",
    "\n",
    "    # randomly get a user\n",
    "    user_id = np.random.choice([i for i in data_store.keys()])\n",
    "    user_items = [i for i in data_store[user_id].keys()]\n",
    "    \n",
    "    # highest rating this user has\n",
    "    highest_rating = max(data_store[user_id].values())\n",
    "    # get highest rated items\n",
    "    highest_rated_items = [x for x in data_store[user_id].keys() if data_store[user_id][x] >= highest_rating]\n",
    "    \n",
    "    # get indices for items this user rated\n",
    "    indices = np.array(user_items)\n",
    "    \n",
    "    # randomly select a particle\n",
    "    random_particle = np.random.choice(range(n_particles))\n",
    "    particle = particles[random_particle]\n",
    "    \n",
    "    # predict a rating only for the items rated by that user\n",
    "    predicted_rating = np.dot(particle[1][\"u\"][user_id, :], particle[1][\"v\"][indices, :].T)\n",
    "    \n",
    "    # get the item id\n",
    "    max_rating_ind = np.argmax(predicted_rating)\n",
    "    item_id = [i for i in data_store[user_id].keys()][max_rating_ind]\n",
    "    \n",
    "    # add to ctr if possible\n",
    "    if item_id in highest_rated_items:\n",
    "        ctr += 1\n",
    "    ctr_hist.append(ctr / (_idx + 1))\n",
    "    \n",
    "    # get the true rating\n",
    "    rating = data_store[user_id][item_id]\n",
    "    \n",
    "    # delete this item from this user\n",
    "    del data_store[user_id][item_id]\n",
    "    \n",
    "    # delete the user from the data store if they have no reviews left\n",
    "    if not data_store[user_id]:\n",
    "        del data_store[user_id]\n",
    "        \n",
    "    error = predicted_rating[max_rating_ind] - rating\n",
    "    se = error ** 2\n",
    "    if _idx % 101 == 0:\n",
    "        ses.append(se)\n",
    "    if _idx % 1000== 0:\n",
    "        print(\"squared error: {:.2f}\".format(se))\n",
    "\n",
    "    # line 17\n",
    "    precision_u_i = []\n",
    "    eta_u_i = []\n",
    "    for particle in particles:\n",
    "        if user_id not in user_history:\n",
    "            precision_u_i.append(np.eye(k))\n",
    "            eta_u_i.append(np.zeros(k))\n",
    "        else:\n",
    "            v_j = particle[1][\"v\"][user_history[user_id][\"item_ids\"], :]\n",
    "            lambda_u_i = 1 / var * \\\n",
    "                np.dot(v_j.T, v_j) + \\\n",
    "                1 / particle[1][\"var_u\"] * np.eye(k)\n",
    "\n",
    "            precision_u_i.append(lambda_u_i)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    v_j,\n",
    "                    np.array(user_history[user_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "            eta_u_i.append(eta.reshape(-1))\n",
    "\n",
    "    # line 18\n",
    "    weights = []\n",
    "    mus = [1 / var * np.dot(np.linalg.inv(lambda_), eta) for lambda_, eta in zip(precision_u_i, eta_u_i)]\n",
    "    for particle, mu, precision in zip(particles, mus, precision_u_i):\n",
    "        v_j = particle[1][\"v\"][item_id, :]\n",
    "        cov = 1 / var + np.dot(np.dot(v_j.T, precision), v_j)\n",
    "        w = np.random.normal(\n",
    "            np.dot(v_j.T, mu),\n",
    "            cov\n",
    "        )\n",
    "        weights.append(w)\n",
    "    normalized_weights = softmax(weights)\n",
    "\n",
    "    # line 19\n",
    "    eff_threshold = 4\n",
    "    Neff = 1/sum([i**2 for i in normalized_weights])\n",
    "    # perform all resampling methods, but only use the resampled particles named ds\n",
    "    if Neff >= eff_threshold:\n",
    "        multis = [np.random.choice(range(n_particles), p=normalized_weights) for _ in range(n_particles)]\n",
    "        multi.append(len(set(multis)))\n",
    "\n",
    "        str_idx = stratified_resample(normalized_weights)\n",
    "        strat.append(len(set(str_idx)))\n",
    "        \n",
    "        # sys_idx\n",
    "        ds = systematic_resample(normalized_weights)\n",
    "        system.append(len(set(ds)))\n",
    "    else:\n",
    "        ds = [i for i in range(n_particles)]\n",
    "\n",
    "    p_prime = [{\"u\": np.copy(particles[d][1][\"u\"]),\n",
    "                \"v\": np.copy(particles[d][1][\"v\"]),\n",
    "                \"var_u\": particles[d][1][\"var_u\"],\n",
    "                \"var_i\": particles[d][1][\"var_i\"]} for d in ds]\n",
    "        \n",
    "    for idx, (particle, precision, e) in enumerate(zip(p_prime, precision_u_i, eta_u_i)):\n",
    "\n",
    "        # line 21\n",
    "        v_j = particle[\"v\"][item_id, :]\n",
    "        add_to_precision = 1 / var * np.dot(v_j.reshape(-1, 1), v_j.reshape(1, -1))\n",
    "        precision += add_to_precision\n",
    "\n",
    "        add_to_eta = rating * v_j\n",
    "        e += add_to_eta\n",
    "\n",
    "        # line 22\n",
    "        sampled_user_vector = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision), e),\n",
    "            np.linalg.inv(precision)\n",
    "        )\n",
    "        \n",
    "        p_prime[idx][\"u\"][user_id, :] = sampled_user_vector\n",
    "\n",
    "        # line 24\n",
    "        if item_id not in item_history:\n",
    "            precision_v_i = np.eye(k)\n",
    "            eta = np.zeros(k)\n",
    "        else:\n",
    "            u_i = particle[\"u\"][item_history[item_id][\"user_ids\"], :]\n",
    "            precision_v_i = 1 / var * \\\n",
    "                np.dot(u_i.T, u_i) + \\\n",
    "                1 / particle[\"var_i\"] * np.eye(k)\n",
    "\n",
    "            eta = np.sum(\n",
    "                np.multiply(\n",
    "                    u_i,\n",
    "                    np.array(item_history[item_id][\"ratings\"]).reshape(-1, 1)\n",
    "                ),\n",
    "                axis=0\n",
    "            )\n",
    "        # line 25\n",
    "        item_sample = np.random.multivariate_normal(\n",
    "            1 / var * np.dot(np.linalg.inv(precision_v_i), eta),\n",
    "            np.linalg.inv(precision_v_i)\n",
    "        )\n",
    "        p_prime[idx][\"v\"][item_id, :] = item_sample\n",
    "        \n",
    "    particles = [(1 / n_particles, particle) for particle in p_prime]\n",
    "\n",
    "    if user_id not in user_history:\n",
    "        user_history[user_id] = {\"item_ids\": [], \"ratings\": []}\n",
    "    if item_id not in item_history:\n",
    "        item_history[item_id] = {\"user_ids\": [], \"ratings\": []}\n",
    "    user_history[user_id][\"item_ids\"].append(item_id)\n",
    "    user_history[user_id][\"ratings\"].append(rating)\n",
    "    item_history[item_id][\"user_ids\"].append(user_id)\n",
    "    item_history[item_id][\"ratings\"].append(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avg num particles Multinomial:\", round(np.mean(multi),2) ) # On average looks like multi uses less particles\n",
    "print(\"Avg num particles Stratfied:\", round(np.mean(strat),2) ) # Followed slightly by stratefied sampling\n",
    "print(\"Avg num particles Systematic:\", round(np.mean(system),2) ) # Systematic uses most -- wonder if should change from multi above to systematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x):\n",
    "    avgs = []\n",
    "    for i, v in enumerate(x):\n",
    "        avgs.append(np.sum(x[:i]) / i)\n",
    "    return avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mses = moving_average(ses)\n",
    "plt.plot(mses)\n",
    "plt.title(\"Final MSE: {:.2f} | Particles={} K={}\".format(mses[-1], n_particles, k))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ctr_hist)\n",
    "plt.title(\"cumulative take rate {:.2f}\".format(ctr_hist[-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_samples = moving_average(system)\n",
    "plt.plot(avg_samples)\n",
    "plt.title(\"Average number of samples at each iteration\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
